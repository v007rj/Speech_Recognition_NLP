{"cells":[{"metadata":{},"cell_type":"markdown","source":"## In this application I have worked on a dataset and applied my NLP framework/algorithm on it to extract certain entites and also applied tagging of the words to certain classes.\n\n- [Step 1](#Step-1:-Read-and-preprocess-the-dataset): \n- [Step 2](#Step-2:-Build-a-Most-Frequent-Class-tagger): \n- [Step 3](#Step-3:-Build-an-HMM-tagger): \n- [Step 4](#Step-4:-[Optional]-Improving-model-performance):"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Jupyter \"magic methods\" -- only need to be run once per kernel restart\n%load_ext autoreload\n%aimport helpers, tests\n%autoreload 1","execution_count":1,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# import python modules -- this cell needs to be run again if you make changes to any of the files\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom IPython.core.display import HTML\nfrom itertools import chain\nfrom collections import Counter, defaultdict\nfrom helpers import show_model, Dataset\nfrom pomegranate import State, HiddenMarkovModel, DiscreteDistribution","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 1: Read and preprocess the dataset\n---\nIn this I will preprocess the dataset\n\nExample from the Brown corpus. \n```\nb100-38532\nPerhaps\tADV\nit\tPRON\nwas\tVERB\nright\tADJ\n;\t.\n;\t.\n\nb100-35577\n...\n```"},{"metadata":{"trusted":false},"cell_type":"code","source":"data = Dataset(\"tags-universal.txt\", \"brown-universal.txt\", train_test_split=0.8)\n\nprint(\"There are {} sentences in the corpus.\".format(len(data)))\nprint(\"There are {} sentences in the training set.\".format(len(data.training_set)))\nprint(\"There are {} sentences in the testing set.\".format(len(data.testing_set)))\n\nassert len(data) == len(data.training_set) + len(data.testing_set), \\\n       \"The number of sentences in the training set + testing set should sum to the number of sentences in the corpus\"","execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":"There are 57340 sentences in the corpus.\nThere are 45872 sentences in the training set.\nThere are 11468 sentences in the testing set.\n"}]},{"metadata":{},"cell_type":"markdown","source":"#### Sentences\n\n`Dataset.sentences` is a dictionary of all sentences in the training corpus, each keyed to a unique sentence identifier. Each `Sentence` is itself an object with two attributes: a tuple of the words in the sentence named `words` and a tuple of the tag corresponding to each word named `tags`."},{"metadata":{"trusted":false},"cell_type":"code","source":"key = 'b100-38532'\nprint(\"Sentence: {}\".format(key))\nprint(\"words:\\n\\t{!s}\".format(data.sentences[key].words))\nprint(\"tags:\\n\\t{!s}\".format(data.sentences[key].tags))","execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"Sentence: b100-38532\nwords:\n\t('Perhaps', 'it', 'was', 'right', ';', ';')\ntags:\n\t('ADV', 'PRON', 'VERB', 'ADJ', '.', '.')\n"}]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n**Note:** The underlying iterable sequence is **unordered** over the sentences in the corpus; it is not guaranteed to return the sentences in a consistent order between calls. Use `Dataset.stream()`, `Dataset.keys`, `Dataset.X`, or `Dataset.Y` attributes if you need ordered access to the data.\n</div>\n\n#### Counting Unique Elements\n\nYou can access the list of unique words (the dataset vocabulary) via `Dataset.vocab` and the unique list of tags via `Dataset.tagset`."},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"There are a total of {} samples of {} unique words in the corpus.\"\n      .format(data.N, len(data.vocab)))\nprint(\"There are {} samples of {} unique words in the training set.\"\n      .format(data.training_set.N, len(data.training_set.vocab)))\nprint(\"There are {} samples of {} unique words in the testing set.\"\n      .format(data.testing_set.N, len(data.testing_set.vocab)))\nprint(\"There are {} words in the test set that are missing in the training set.\"\n      .format(len(data.testing_set.vocab - data.training_set.vocab)))\n\nassert data.N == data.training_set.N + data.testing_set.N, \\\n       \"The number of training + test samples should sum to the total number of samples\"","execution_count":5,"outputs":[{"name":"stdout","output_type":"stream","text":"There are a total of 1161192 samples of 56057 unique words in the corpus.\nThere are 928458 samples of 50536 unique words in the training set.\nThere are 232734 samples of 25112 unique words in the testing set.\nThere are 5521 words in the test set that are missing in the training set.\n"}]},{"metadata":{},"cell_type":"markdown","source":"#### Accessing word and tag Sequences\nThe `Dataset.X` and `Dataset.Y` attributes provide access to ordered collections of matching word and tag sequences for each sentence in the dataset."},{"metadata":{"trusted":false},"cell_type":"code","source":"# accessing words with Dataset.X and tags with Dataset.Y \nfor i in range(2):    \n    print(\"Sentence {}:\".format(i + 1), data.X[i])\n    print()\n    print(\"Labels {}:\".format(i + 1), data.Y[i])\n    print()","execution_count":6,"outputs":[{"name":"stdout","output_type":"stream","text":"Sentence 1: ('Mr.', 'Podger', 'had', 'thanked', 'him', 'gravely', ',', 'and', 'now', 'he', 'made', 'use', 'of', 'the', 'advice', '.')\n\nLabels 1: ('NOUN', 'NOUN', 'VERB', 'VERB', 'PRON', 'ADV', '.', 'CONJ', 'ADV', 'PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n\nSentence 2: ('But', 'there', 'seemed', 'to', 'be', 'some', 'difference', 'of', 'opinion', 'as', 'to', 'how', 'far', 'the', 'board', 'should', 'go', ',', 'and', 'whose', 'advice', 'it', 'should', 'follow', '.')\n\nLabels 2: ('CONJ', 'PRT', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'ADP', 'ADV', 'ADV', 'DET', 'NOUN', 'VERB', 'VERB', '.', 'CONJ', 'DET', 'NOUN', 'PRON', 'VERB', 'VERB', '.')\n\n"}]},{"metadata":{},"cell_type":"markdown","source":"#### Accessing (word, tag) Samples\nThe `Dataset.stream()` method returns an iterator that chains together every pair of (word, tag) entries across all sentences in the entire corpus."},{"metadata":{"trusted":false},"cell_type":"code","source":"# use Dataset.stream() (word, tag) samples for the entire corpus\nprint(\"\\nStream (word, tag) pairs:\\n\")\nfor i, pair in enumerate(data.stream()):\n    print(\"\\t\", pair)\n    if i > 5: break","execution_count":7,"outputs":[{"name":"stdout","output_type":"stream","text":"\nStream (word, tag) pairs:\n\n\t ('Mr.', 'NOUN')\n\t ('Podger', 'NOUN')\n\t ('had', 'VERB')\n\t ('thanked', 'VERB')\n\t ('him', 'PRON')\n\t ('gravely', 'ADV')\n\t (',', '.')\n"}]},{"metadata":{},"cell_type":"markdown","source":"\nFor both our baseline tagger and the HMM model we'll build, we need to estimate the frequency of tags & words from the frequency counts of observations in the training corpus. In the next several cells you will complete functions to compute the counts of several sets of counts. "},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Build a Most Frequent Class tagger\n---\n\nPerhaps the simplest tagger (and a good baseline for tagger performance) is to simply choose the tag most frequently assigned to each word. This \"most frequent class\" tagger inspects each observed word in the sequence and assigns it the label that was most often assigned to that word in the corpus."},{"metadata":{},"cell_type":"markdown","source":"### IMPLEMENTATION: Pair Counts\n\nComplete the function below that computes the joint frequency counts for two input sequences."},{"metadata":{"trusted":false},"cell_type":"code","source":"from collections import Counter, defaultdict\n\ndef pair_counts(tags, words):\n    \"\"\"Return a dictionary keyed to each unique value in the first sequence list\n    that counts the number of occurrences of the corresponding value from the\n    second sequences list.\n    \n    For example, if sequences_A is tags and sequences_B is the corresponding\n    words, then if 1244 sequences contain the word \"time\" tagged as a NOUN, then\n    you should return a dictionary such that pair_counts[NOUN][time] == 1244\n    \"\"\"\n    # TODO: Finish this function!\n    \n    d = defaultdict(lambda: defaultdict(int))\n\n    for tag,word in zip(tags,words):\n        d[tag][word]+=1\n    \n   \n    return d\n    raise NotImplementedError\n\n\n# Calculate C(t_i, w_i)\n#This allows use to strip the data stream into 2 parts a.k.a tags and words\ntags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]\nwords = [word for i, (word, tag) in enumerate(data.training_set.stream())]\n#testing\nemission_counts = pair_counts(tags, words)\n\n\n\nassert len(emission_counts) == 12, \\\n       \"Uh oh. There should be 12 tags in your dictionary.\"\nassert max(emission_counts[\"NOUN\"], key=emission_counts[\"NOUN\"].get) == 'time', \\\n       \"Hmmm...'time' is expected to be the most common NOUN.\"\nHTML('<div class=\"alert alert-block alert-success\">Your emission counts look good!</div>')","execution_count":8,"outputs":[{"data":{"text/html":"<div class=\"alert alert-block alert-success\">Your emission counts look good!</div>","text/plain":"<IPython.core.display.HTML object>"},"execution_count":8,"metadata":{},"output_type":"execute_result"}]},{"metadata":{},"cell_type":"markdown","source":"### IMPLEMENTATION: Most Frequent Class Tagger\n\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create a lookup table mfc_table where mfc_table[word] contains the tag label most frequently assigned to that word\nfrom collections import namedtuple\n\nFakeState = namedtuple(\"FakeState\", \"name\")\n\nclass MFCTagger:\n    # NOTE: You should not need to modify this class or any of its methods\n    missing = FakeState(name=\"<MISSING>\")\n    \n    def __init__(self, table):\n        self.table = defaultdict(lambda: MFCTagger.missing)\n        self.table.update({word: FakeState(name=tag) for word, tag in table.items()})\n        \n    def viterbi(self, seq):\n        \"\"\"This method simplifies predictions by matching the Pomegranate viterbi() interface\"\"\"\n        return 0., list(enumerate([\"<start>\"] + [self.table[w] for w in seq] + [\"<end>\"]))\n\n\n# TODO: calculate the frequency of each tag being assigned to each word (hint: similar, but not\n# the same as the emission probabilities) and use it to fill the mfc_table\n\ntags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]\nwords = [word for i, (word, tag) in enumerate(data.training_set.stream())]\n#Since this is the word_counts we will pass first words and then counts\nword_counts = pair_counts(words,tags)\n\nmfc_table = dict((word, max(tags.keys(), key=lambda key: tags[key])) for word, tags in word_counts.items())\n\n# DO NOT MODIFY BELOW THIS LINE\nmfc_model = MFCTagger(mfc_table) # Create a Most Frequent Class tagger instance\n\nassert len(mfc_table) == len(data.training_set.vocab), \"\"\nassert all(k in data.training_set.vocab for k in mfc_table.keys()), \"\"\nassert sum(int(k not in mfc_table) for k in data.testing_set.vocab) == 5521, \"\"\nHTML('<div class=\"alert alert-block alert-success\">Your MFC tagger has all the correct words!</div>')","execution_count":9,"outputs":[{"data":{"text/html":"<div class=\"alert alert-block alert-success\">Your MFC tagger has all the correct words!</div>","text/plain":"<IPython.core.display.HTML object>"},"execution_count":9,"metadata":{},"output_type":"execute_result"}]},{"metadata":{},"cell_type":"markdown","source":"### Making Predictions with a Model\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"def replace_unknown(sequence):\n    \"\"\"Return a copy of the input sequence where each unknown word is replaced\n    by the literal string value 'nan'. Pomegranate will ignore these values\n    during computation.\n    \"\"\"\n    return [w if w in data.training_set.vocab else 'nan' for w in sequence]\n\ndef simplify_decoding(X, model):\n    \"\"\"X should be a 1-D sequence of observations for the model to predict\"\"\"\n    _, state_path = model.viterbi(replace_unknown(X))\n    return [state[1].name for state in state_path[1:-1]]  # do not show the start/end state predictions","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Example Decoding Sequences with MFC Tagger"},{"metadata":{"trusted":false},"cell_type":"code","source":"for key in data.testing_set.keys[:3]:\n    print(\"Sentence Key: {}\\n\".format(key))\n    print(\"Predicted labels:\\n-----------------\")\n    print(simplify_decoding(data.sentences[key].words, mfc_model))\n    print()\n    print(\"Actual labels:\\n--------------\")\n    print(data.sentences[key].tags)\n    print(\"\\n\")","execution_count":11,"outputs":[{"name":"stdout","output_type":"stream","text":"Sentence Key: b100-28144\n\nPredicted labels:\n-----------------\n['CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.']\n\nActual labels:\n--------------\n('CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.')\n\n\nSentence Key: b100-23146\n\nPredicted labels:\n-----------------\n['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.']\n\nActual labels:\n--------------\n('PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n\n\nSentence Key: b100-35462\n\nPredicted labels:\n-----------------\n['DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', '<MISSING>', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADV', 'NOUN', '.']\n\nActual labels:\n--------------\n('DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.')\n\n\n"}]},{"metadata":{},"cell_type":"markdown","source":"### Evaluating Model Accuracy\n\nThe function below will evaluate the accuracy of the MFC tagger on the collection of all sentences from a text corpus. "},{"metadata":{"trusted":false},"cell_type":"code","source":"def accuracy(X, Y, model):\n    \"\"\"Calculate the prediction accuracy by using the model to decode each sequence\n    in the input X and comparing the prediction with the true labels in Y.\n    \n    The X should be an array whose first dimension is the number of sentences to test,\n    and each element of the array should be an iterable of the words in the sequence.\n    The arrays X and Y should have the exact same shape.\n    \n    X = [(\"See\", \"Spot\", \"run\"), (\"Run\", \"Spot\", \"run\", \"fast\"), ...]\n    Y = [(), (), ...]\n    \"\"\"\n    correct = total_predictions = 0\n    for observations, actual_tags in zip(X, Y):\n        \n        # The model.viterbi call in simplify_decoding will return None if the HMM\n        # raises an error (for example, if a test sentence contains a word that\n        # is out of vocabulary for the training set). Any exception counts the\n        # full sentence as an error (which makes this a conservative estimate).\n        try:\n            most_likely_tags = simplify_decoding(observations, model)\n            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n        except:\n            pass\n        total_predictions += len(observations)\n    return correct / total_predictions","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluate the accuracy of the MFC tagger\nRun the next cell to evaluate the accuracy of the tagger on the training and test corpus."},{"metadata":{"trusted":false},"cell_type":"code","source":"mfc_training_acc = accuracy(data.training_set.X, data.training_set.Y, mfc_model)\nprint(\"training accuracy mfc_model: {:.2f}%\".format(100 * mfc_training_acc))\n\nmfc_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, mfc_model)\nprint(\"testing accuracy mfc_model: {:.2f}%\".format(100 * mfc_testing_acc))\n\nassert mfc_training_acc >= 0.955, \"Uh oh. Your MFC accuracy on the training set doesn't look right.\"\nassert mfc_testing_acc >= 0.925, \"Uh oh. Your MFC accuracy on the testing set doesn't look right.\"\nHTML('<div class=\"alert alert-block alert-success\">Your MFC tagger accuracy looks correct!</div>')","execution_count":13,"outputs":[{"name":"stdout","output_type":"stream","text":"training accuracy mfc_model: 95.72%\ntesting accuracy mfc_model: 93.01%\n"},{"data":{"text/html":"<div class=\"alert alert-block alert-success\">Your MFC tagger accuracy looks correct!</div>","text/plain":"<IPython.core.display.HTML object>"},"execution_count":13,"metadata":{},"output_type":"execute_result"}]},{"metadata":{},"cell_type":"markdown","source":"## Step 3: Build an HMM tagger\n---\n"},{"metadata":{},"cell_type":"markdown","source":"### IMPLEMENTATION: Unigram Counts\n\nComplete the function below to estimate the co-occurrence frequency of each symbol over all of the input sequences. The unigram probabilities in our HMM model are estimated from the formula below, where N is the total number of samples in the input. (You only need to compute the counts for now.)\n\n$$P(tag_1) = \\frac{C(tag_1)}{N}$$"},{"metadata":{"trusted":false},"cell_type":"code","source":"def unigram_counts(sequences):\n    \"\"\"Return a dictionary keyed to each unique value in the input sequence list that\n    counts the number of occurrences of the value in the sequences list. The sequences\n    collection should be a 2-dimensional array.\n    \n    For example, if the tag NOUN appears 275558 times over all the input sequences,\n    then you should return a dictionary such that your_unigram_counts[NOUN] == 275558.\n    \"\"\"\n    # TODO: Finish this function!\n    return Counter(sequences)\n    raise NotImplementedError\n\n# TODO: call unigram_counts with a list of tag sequences from the training set\ntags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]\ntag_unigrams = unigram_counts(tags)\n\nassert set(tag_unigrams.keys()) == data.training_set.tagset, \\\n       \"Uh oh. It looks like your tag counts doesn't include all the tags!\"\nassert min(tag_unigrams, key=tag_unigrams.get) == 'X', \\\n       \"Hmmm...'X' is expected to be the least common class\"\nassert max(tag_unigrams, key=tag_unigrams.get) == 'NOUN', \\\n       \"Hmmm...'NOUN' is expected to be the most common class\"\nHTML('<div class=\"alert alert-block alert-success\">Your tag unigrams look good!</div>')","execution_count":14,"outputs":[{"data":{"text/html":"<div class=\"alert alert-block alert-success\">Your tag unigrams look good!</div>","text/plain":"<IPython.core.display.HTML object>"},"execution_count":14,"metadata":{},"output_type":"execute_result"}]},{"metadata":{},"cell_type":"markdown","source":"### IMPLEMENTATION: Bigram Counts\n\nComplete the function below to estimate the co-occurrence frequency of each pair of symbols in each of the input sequences. These counts are used in the HMM model to estimate the bigram probability of two tags from the frequency counts according to the formula: $$P(tag_2|tag_1) = \\frac{C(tag_2|tag_1)}{C(tag_2)}$$\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"def bigram_counts(sequences):\n    \"\"\"Return a dictionary keyed to each unique PAIR of values in the input sequences\n    list that counts the number of occurrences of pair in the sequences list. The input\n    should be a 2-dimensional array.\n    \n    For example, if the pair of tags (NOUN, VERB) appear 61582 times, then you should\n    return a dictionary such that your_bigram_counts[(NOUN, VERB)] == 61582\n    \"\"\"\n\n    # TODO: Finish this function!\n    d = Counter(sequences)\n    return d\n    raise NotImplementedError\n\n# TODO: call bigram_counts with a list of tag sequences from the training set\ntags = [tag for i, (word, tag) in enumerate(data.stream())]\no = [(tags[i],tags[i+1]) for i in range(0,len(tags)-2,2)]\ntag_bigrams = bigram_counts(o)\n\nassert len(tag_bigrams) == 144, \\\n       \"Uh oh. There should be 144 pairs of bigrams (12 tags x 12 tags)\"\nassert min(tag_bigrams, key=tag_bigrams.get) in [('X', 'NUM'), ('PRON', 'X')], \\\n       \"Hmmm...The least common bigram should be one of ('X', 'NUM') or ('PRON', 'X').\"\nassert max(tag_bigrams, key=tag_bigrams.get) in [('DET', 'NOUN')], \\\n       \"Hmmm...('DET', 'NOUN') is expected to be the most common bigram.\"\nHTML('<div class=\"alert alert-block alert-success\">Your tag bigrams look good!</div>')","execution_count":15,"outputs":[{"data":{"text/html":"<div class=\"alert alert-block alert-success\">Your tag bigrams look good!</div>","text/plain":"<IPython.core.display.HTML object>"},"execution_count":15,"metadata":{},"output_type":"execute_result"}]},{"metadata":{},"cell_type":"markdown","source":"### IMPLEMENTATION: Sequence Starting Counts\nComplete the code below to estimate the bigram probabilities of a sequence starting with each tag."},{"metadata":{"trusted":false},"cell_type":"code","source":"def starting_counts(sequences):\n    \"\"\"Return a dictionary keyed to each unique value in the input sequences list\n    that counts the number of occurrences where that value is at the beginning of\n    a sequence.\n    \n    For example, if 8093 sequences start with NOUN, then you should return a\n    dictionary such that your_starting_counts[NOUN] == 8093\n    \"\"\"\n    # TODO: Finish this function!\n    d = Counter(sequences)\n    return d\n    raise NotImplementedError\n\n# TODO: Calculate the count of each tag starting a sequence\ntags = [tag for i, (word, tag) in enumerate(data.stream())]\nstarts_tag = [i[0] for i in data.Y]\ntag_starts = starting_counts(starts_tag)\n\nassert len(tag_starts) == 12, \"Uh oh. There should be 12 tags in your dictionary.\"\nassert min(tag_starts, key=tag_starts.get) == 'X', \"Hmmm...'X' is expected to be the least common starting bigram.\"\nassert max(tag_starts, key=tag_starts.get) == 'DET', \"Hmmm...'DET' is expected to be the most common starting bigram.\"\nHTML('<div class=\"alert alert-block alert-success\">Your starting tag counts look good!</div>')","execution_count":16,"outputs":[{"data":{"text/html":"<div class=\"alert alert-block alert-success\">Your starting tag counts look good!</div>","text/plain":"<IPython.core.display.HTML object>"},"execution_count":16,"metadata":{},"output_type":"execute_result"}]},{"metadata":{},"cell_type":"markdown","source":"### IMPLEMENTATION: Sequence Ending Counts\nComplete the function below to estimate the bigram probabilities of a sequence ending with each tag."},{"metadata":{"trusted":false},"cell_type":"code","source":"def ending_counts(sequences):\n    \"\"\"Return a dictionary keyed to each unique value in the input sequences list\n    that counts the number of occurrences where that value is at the end of\n    a sequence.\n    \n    For example, if 18 sequences end with DET, then you should return a\n    dictionary such that your_starting_counts[DET] == 18\n    \"\"\"\n    d = Counter(sequences)\n    return d\n    raise NotImplementedError\n\n# TODO: Calculate the count of each tag ending a sequence\nends_tag = [i[len(i)-1] for i in data.Y]\ntag_ends = ending_counts(ends_tag)\n\nassert len(tag_ends) == 12, \"Uh oh. There should be 12 tags in your dictionary.\"\nassert min(tag_ends, key=tag_ends.get) in ['X', 'CONJ'], \"Hmmm...'X' or 'CONJ' should be the least common ending bigram.\"\nassert max(tag_ends, key=tag_ends.get) == '.', \"Hmmm...'.' is expected to be the most common ending bigram.\"\nHTML('<div class=\"alert alert-block alert-success\">Your ending tag counts look good!</div>')","execution_count":17,"outputs":[{"data":{"text/html":"<div class=\"alert alert-block alert-success\">Your ending tag counts look good!</div>","text/plain":"<IPython.core.display.HTML object>"},"execution_count":17,"metadata":{},"output_type":"execute_result"}]},{"metadata":{},"cell_type":"markdown","source":"### IMPLEMENTATION: Basic HMM Tagger\nUse the tag unigrams and bigrams calculated above to construct a hidden Markov tagger.\n\n- Add one state per tag\n    - The emission distribution at each state should be estimated with the formula: $P(w|t) = \\frac{C(t, w)}{C(t)}$\n- Add an edge from the starting state `basic_model.start` to each tag\n    - The transition probability should be estimated with the formula: $P(t|start) = \\frac{C(start, t)}{C(start)}$\n- Add an edge from each tag to the end state `basic_model.end`\n    - The transition probability should be estimated with the formula: $P(end|t) = \\frac{C(t, end)}{C(t)}$\n- Add an edge between _every_ pair of tags\n    - The transition probability should be estimated with the formula: $P(t_2|t_1) = \\frac{C(t_1, t_2)}{C(t_1)}$"},{"metadata":{"trusted":false},"cell_type":"code","source":"basic_model = HiddenMarkovModel(name=\"base-hmm-tagger\")\n\n# TODO: create states with emission probability distributions P(word | tag) and add to the model\n# (Hint: you may need to loop & create/add new states)\n#getting tags and words\ntags = [tag for i, (word, tag) in enumerate(data.stream())]\nwords = [word for i, (word, tag) in enumerate(data.stream())]\n\n#Calling functions\ntags_count=unigram_counts(tags)\ntag_words_count=pair_counts(tags,words)\n\nstarting_tag = [i[0] for i in data.Y]\nending_tag = [i[-1] for i in data.Y]\n\nstarting_tag_counts = starting_counts(starting_tag)#the number of times a tag occured at the start\nending_tag_counts = ending_counts(ending_tag)      #the number of times a tag occured at the end\n\nstates = []\nfor tag, words_dict in tag_words_count.items():\n    total = float(sum(words_dict.values()))\n    distribution = {word: count/total for word, count in words_dict.items()}\n    tag_emissions = DiscreteDistribution(distribution)\n    tag_state = State(tag_emissions, name=tag)\n    states.append(tag_state)\n\n\nbasic_model.add_states()    \n    \n\nstart_prob={} #a dict to store \n\nfor tag in tags:\n    start_prob[tag]=starting_tag_counts[tag]/tags_count[tag]\n\nfor tag_state in states :\n    basic_model.add_transition(basic_model.start,tag_state,start_prob[tag_state.name])    \n\nend_prob={}\n\nfor tag in tags:\n    end_prob[tag]=ending_tag_counts[tag]/tags_count[tag]\nfor tag_state in states :\n    basic_model.add_transition(tag_state,basic_model.end,end_prob[tag_state.name])\n    \n\n# TODO: add edges between states for the observed transition frequencies P(tag_i | tag_i-1)\n# (Hint: you may need to loop & add transitions\n# basic_model.add_transition()\n\ntransition_prob_pair={}\n\nfor key in tag_bigrams.keys():\n    transition_prob_pair[key]=tag_bigrams.get(key)/tags_count[key[0]]\nfor tag_state in states :\n    for next_tag_state in states :\n        basic_model.add_transition(tag_state,next_tag_state,transition_prob_pair[(tag_state.name,next_tag_state.name)])\n\n\n# NOTE: YOU SHOULD NOT NEED TO MODIFY ANYTHING BELOW THIS LINE\n# finalize the model\nbasic_model.bake()\n\nassert all(tag in set(s.name for s in basic_model.states) for tag in data.training_set.tagset), \\\n       \"Every state in your network should use the name of the associated tag, which must be one of the training set tags.\"\nassert basic_model.edge_count() == 168, \\\n       (\"Your network should have an edge from the start node to each state, one edge between every \" +\n        \"pair of tags (states), and an edge from each state to the end node.\")\nHTML('<div class=\"alert alert-block alert-success\">Your HMM network topology looks good!</div>')","execution_count":19,"outputs":[{"data":{"text/html":"<div class=\"alert alert-block alert-success\">Your HMM network topology looks good!</div>","text/plain":"<IPython.core.display.HTML object>"},"execution_count":19,"metadata":{},"output_type":"execute_result"}]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"hmm_training_acc = accuracy(data.training_set.X, data.training_set.Y, basic_model)\nprint(\"training accuracy basic hmm model: {:.2f}%\".format(100 * hmm_training_acc))\n\nhmm_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, basic_model)\nprint(\"testing accuracy basic hmm model: {:.2f}%\".format(100 * hmm_testing_acc))\n\nassert hmm_training_acc > 0.97, \"Uh oh. Your HMM accuracy on the training set doesn't look right.\"\nassert hmm_testing_acc > 0.955, \"Uh oh. Your HMM accuracy on the testing set doesn't look right.\"\nHTML('<div class=\"alert alert-block alert-success\">Your HMM tagger accuracy looks correct! Congratulations, you\\'ve finished the project.</div>')","execution_count":20,"outputs":[{"name":"stdout","output_type":"stream","text":"training accuracy basic hmm model: 97.49%\ntesting accuracy basic hmm model: 96.09%\n"},{"data":{"text/html":"<div class=\"alert alert-block alert-success\">Your HMM tagger accuracy looks correct! Congratulations, you've finished the project.</div>","text/plain":"<IPython.core.display.HTML object>"},"execution_count":20,"metadata":{},"output_type":"execute_result"}]},{"metadata":{},"cell_type":"markdown","source":"### Example Decoding Sequences with the HMM Tagger"},{"metadata":{"trusted":false},"cell_type":"code","source":"for key in data.testing_set.keys[:3]:\n    print(\"Sentence Key: {}\\n\".format(key))\n    print(\"Predicted labels:\\n-----------------\")\n    print(simplify_decoding(data.sentences[key].words, basic_model))\n    print()\n    print(\"Actual labels:\\n--------------\")\n    print(data.sentences[key].tags)\n    print(\"\\n\")","execution_count":21,"outputs":[{"name":"stdout","output_type":"stream","text":"Sentence Key: b100-28144\n\nPredicted labels:\n-----------------\n['CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.']\n\nActual labels:\n--------------\n('CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.')\n\n\nSentence Key: b100-23146\n\nPredicted labels:\n-----------------\n['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.']\n\nActual labels:\n--------------\n('PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n\n\nSentence Key: b100-35462\n\nPredicted labels:\n-----------------\n['DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.']\n\nActual labels:\n--------------\n('DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.')\n\n\n"}]},{"metadata":{},"cell_type":"markdown","source":"\n## Finishing the project\n---\n\n<div class=\"alert alert-block alert-info\">\n**Note:** **SAVE YOUR NOTEBOOK**, then run the next cell to generate an HTML copy. You will zip & submit both this file and the HTML copy for review.\n</div>"},{"metadata":{"trusted":false},"cell_type":"code","source":"!!jupyter nbconvert *.ipynb","execution_count":23,"outputs":[{"data":{"text/plain":"['[NbConvertApp] Converting notebook HMM Tagger.ipynb to html',\n '[NbConvertApp] Writing 359078 bytes to HMM Tagger.html',\n '[NbConvertApp] Converting notebook HMM warmup (optional).ipynb to html',\n '[NbConvertApp] Writing 308701 bytes to HMM warmup (optional).html']"},"execution_count":23,"metadata":{},"output_type":"execute_result"}]},{"metadata":{},"cell_type":"markdown","source":"## Step 4:  Improving model performance\n---\nThere are additional enhancements that can be incorporated into your tagger that improve performance on larger tagsets where the data sparsity problem is more significant. The data sparsity problem arises because the same amount of data split over more tags means there will be fewer samples in each tag, and there will be more missing data  tags that have zero occurrences in the data. The techniques in this section are optional.\n\n- [Laplace Smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) (pseudocounts)\n    Laplace smoothing is a technique where you add a small, non-zero value to all observed counts to offset for unobserved values.\n\n- Backoff Smoothing\n    Another smoothing technique is to interpolate between n-grams for missing data. This method is more effective than Laplace smoothing at combatting the data sparsity problem. Refer to chapters 4, 9, and 10 of the [Speech & Language Processing](https://web.stanford.edu/~jurafsky/slp3/) book for more information.\n\n- Extending to Trigrams\n    HMM taggers have achieved better than 96% accuracy on this dataset with the full Penn treebank tagset using an architecture described in [this](http://www.coli.uni-saarland.de/~thorsten/publications/Brants-ANLP00.pdf) paper. Altering your HMM to achieve the same performance would require implementing deleted interpolation (described in the paper), incorporating trigram probabilities in your frequency tables, and re-implementing the Viterbi algorithm to consider three consecutive states instead of two.\n\n### Obtain the Brown Corpus with a Larger Tagset\nRun the code below to download a copy of the brown corpus with the full NLTK tagset. You will need to research the available tagset information in the NLTK docs and determine the best way to extract the subset of NLTK tags you want to explore. If you write the following the format specified in Step 1, then you can reload the data using all of the code above for comparison.\n\nRefer to [Chapter 5](http://www.nltk.org/book/ch05.html) of the NLTK book for more information on the available tagsets."},{"metadata":{"trusted":false},"cell_type":"code","source":"import nltk\nfrom nltk import pos_tag, word_tokenize\nfrom nltk.corpus import brown\n\nnltk.download('brown')\ntraining_corpus = nltk.corpus.brown\ntraining_corpus.tagged_sents()[0]","execution_count":22,"outputs":[{"name":"stdout","output_type":"stream","text":"[nltk_data] Downloading package brown to /root/nltk_data...\n[nltk_data]   Unzipping corpora/brown.zip.\n"},{"data":{"text/plain":"[('The', 'AT'),\n ('Fulton', 'NP-TL'),\n ('County', 'NN-TL'),\n ('Grand', 'JJ-TL'),\n ('Jury', 'NN-TL'),\n ('said', 'VBD'),\n ('Friday', 'NR'),\n ('an', 'AT'),\n ('investigation', 'NN'),\n ('of', 'IN'),\n (\"Atlanta's\", 'NP$'),\n ('recent', 'JJ'),\n ('primary', 'NN'),\n ('election', 'NN'),\n ('produced', 'VBD'),\n ('``', '``'),\n ('no', 'AT'),\n ('evidence', 'NN'),\n (\"''\", \"''\"),\n ('that', 'CS'),\n ('any', 'DTI'),\n ('irregularities', 'NNS'),\n ('took', 'VBD'),\n ('place', 'NN'),\n ('.', '.')]"},"execution_count":22,"metadata":{},"output_type":"execute_result"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}